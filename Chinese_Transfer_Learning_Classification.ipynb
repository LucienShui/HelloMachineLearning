{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chinese Transfer Learning Classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfTcM1H8_rmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWH3U2KlBYWs",
        "colab_type": "code",
        "outputId": "67a2fb34-a037-43e1-9f11-55b99c6fa8b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip3 install kashgari"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kashgari\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/50/4db8af2a78e12e48c557c7c475842bd30f2988bf4e751bf501911d8881f7/kashgari-1.0.0-py3-none-any.whl (75kB)\n",
            "\r\u001b[K     |████▍                           | 10kB 17.5MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 20kB 897kB/s eta 0:00:01\r\u001b[K     |█████████████                   | 30kB 1.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 40kB 895kB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 51kB 1.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 61kB 1.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 71kB 1.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 1.5MB/s \n",
            "\u001b[?25hCollecting numpy==1.16.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/2d/e4656149cbadd3a8a0369fcd1a9c7d61cc7b87b3903b85389c70c989a696/numpy-1.16.4-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 491kB/s \n",
            "\u001b[?25hCollecting seqeval==0.0.10\n",
            "  Downloading https://files.pythonhosted.org/packages/55/dd/3bf1c646c310daabae47fceb84ea9ab66df7f518a31a89955290d82b8100/seqeval-0.0.10-py3-none-any.whl\n",
            "Requirement already satisfied: scikit-learn>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from kashgari) (0.21.3)\n",
            "Collecting keras-bert>=0.50.0\n",
            "  Downloading https://files.pythonhosted.org/packages/df/fe/bf46de1ef9d1395cd735d8df5402f5d837ef82cfd348a252ad8f32feeaef/keras-bert-0.80.0.tar.gz\n",
            "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from kashgari) (0.25.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from kashgari) (2.8.0)\n",
            "Requirement already satisfied: gensim>=3.5.0 in /usr/local/lib/python3.6/dist-packages (from kashgari) (3.6.0)\n",
            "Collecting keras-gpt-2>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/df/19/d11eac066ffcb61ec9edd23c02e4651eaa31f1f67c167a636dd90b6142a4/keras-gpt-2-0.14.0.tar.gz\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval==0.0.10->kashgari) (2.2.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.1->kashgari) (1.3.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.1->kashgari) (0.14.0)\n",
            "Collecting keras-transformer>=0.30.0\n",
            "  Downloading https://files.pythonhosted.org/packages/0a/57/496b1eab888171b0801a0a44d3245a7874b8d1cc04c1fbfdbb5e3327fc7a/keras-transformer-0.31.0.tar.gz\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->kashgari) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->kashgari) (2.6.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->kashgari) (1.12.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.5.0->kashgari) (1.8.4)\n",
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 22.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval==0.0.10->kashgari) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval==0.0.10->kashgari) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval==0.0.10->kashgari) (3.13)\n",
            "Collecting keras-pos-embd>=0.10.0\n",
            "  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n",
            "Collecting keras-multi-head>=0.22.0\n",
            "  Downloading https://files.pythonhosted.org/packages/40/3e/d0a64bb2ac5217928effe4507c26bbd19b86145d16a1948bc2d4f4c6338a/keras-multi-head-0.22.0.tar.gz\n",
            "Collecting keras-layer-normalization>=0.12.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n",
            "Collecting keras-embed-sim>=0.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/20/735fd53f6896e2af63af47e212601c1b8a7a80d00b6126c388c9d1233892/keras-embed-sim-0.7.0.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.5.0->kashgari) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.5.0->kashgari) (1.10.4)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.5.0->kashgari) (2.49.0)\n",
            "Collecting keras-self-attention==0.41.0\n",
            "  Downloading https://files.pythonhosted.org/packages/1b/1c/01599219bef7266fa43b3316e4f55bcb487734d3bafdc60ffd564f3cfe29/keras-self-attention-0.41.0.tar.gz\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=3.5.0->kashgari) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=3.5.0->kashgari) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=3.5.0->kashgari) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=3.5.0->kashgari) (3.0.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.4 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.5.0->kashgari) (1.13.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.5.0->kashgari) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.5.0->kashgari) (0.9.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.4->boto3->smart-open>=1.2.1->gensim>=3.5.0->kashgari) (0.15.2)\n",
            "Building wheels for collected packages: keras-bert, keras-gpt-2, keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.80.0-cp36-none-any.whl size=37923 sha256=1dadc1fef0e66bed7c8a03311ecc0cccf00468e0508048c75a40770e63b96aec\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/dc/87/3260cb91f3aa32c0f85c5375429a30c8fd988bbb48f5ee21b0\n",
            "  Building wheel for keras-gpt-2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-gpt-2: filename=keras_gpt_2-0.14.0-cp36-none-any.whl size=10525 sha256=4e03f5d024ff91aee4b70078e196717eb9c5fe76a8612261f762d341a8196d2c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/d8/06/ba8216a77a55b8ba4a5c3932c7df93e87eeaea83ced27822aa\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.31.0-cp36-none-any.whl size=13385 sha256=95abbf21c2b6ec7e033cab8ea5c5e54537ac5a30bbb27a6850bd9dbc342e3e13\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/c5/9a/5a5130240be614a7a6fa786765d7692ae97f82601e2161bb56\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp36-none-any.whl size=7553 sha256=8c0b4040ef2912f1f9598b8b6eb92325686c43aaba1feab10bfbf8a82a0f2ec1\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.22.0-cp36-none-any.whl size=15371 sha256=2fa5370da6491065bfafd53d59bf523066a98314b9adc70f5e922fcdc7bf1c8d\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/df/3f/81b36f41b66e6a9cd69224c70a737de2bb6b2f7feb3272c25e\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp36-none-any.whl size=5268 sha256=252ae969ce9a0f9c955101a2cad21d467c8b4f10bf2a83e4b96a636b9ea11d1a\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp36-none-any.whl size=5624 sha256=469f9c8bc144860396ce54d5097aff363f0bf0e2e8c4f93a3d74c0fc64b92a24\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.7.0-cp36-none-any.whl size=4676 sha256=beefc5215baa17fbf8acf6c25326a6742687616a2295d1978e32533a2dbf761f\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/bc/b1/b0c45cee4ca2e6c86586b0218ffafe7f0703c6d07fdf049866\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.41.0-cp36-none-any.whl size=17290 sha256=a7907b1688d4f5733a35439587509a328abf34c3b08f2f620282af66697c14d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/dc/17/84258b27a04cd38ac91998abe148203720ca696186635db694\n",
            "Successfully built keras-bert keras-gpt-2 keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, seqeval, keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert, regex, keras-gpt-2, kashgari\n",
            "  Found existing installation: numpy 1.17.3\n",
            "    Uninstalling numpy-1.17.3:\n",
            "      Successfully uninstalled numpy-1.17.3\n",
            "Successfully installed kashgari-1.0.0 keras-bert-0.80.0 keras-embed-sim-0.7.0 keras-gpt-2-0.14.0 keras-layer-normalization-0.14.0 keras-multi-head-0.22.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.41.0 keras-transformer-0.31.0 numpy-1.16.4 regex-2019.11.1 seqeval-0.0.10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAw5dnxqsq8e",
        "colab_type": "code",
        "outputId": "ec806303-48d1-4db7-b10a-06dc1e6b0af3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "# 下载数据集\n",
        "!mkdir -p ~/datasets/\n",
        "!wget \"https://raw.githubusercontent.com/SophonPlus/ChineseNlpCorpus/master/datasets/ChnSentiCorp_htl_all/ChnSentiCorp_htl_all.csv\" -O ~/datasets/data.csv\n",
        "!ls ~/datasets"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-04 03:00:32--  https://raw.githubusercontent.com/SophonPlus/ChineseNlpCorpus/master/datasets/ChnSentiCorp_htl_all/ChnSentiCorp_htl_all.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2904858 (2.8M) [text/plain]\n",
            "Saving to: ‘/root/datasets/data.csv’\n",
            "\n",
            "\r/root/datasets/data   0%[                    ]       0  --.-KB/s               \r/root/datasets/data 100%[===================>]   2.77M  16.8MB/s    in 0.2s    \n",
            "\n",
            "2019-11-04 03:00:33 (16.8 MB/s) - ‘/root/datasets/data.csv’ saved [2904858/2904858]\n",
            "\n",
            "data.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPOtkUq_A6aL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "409f6f87-968b-4420-cc61-c6679172d6db"
      },
      "source": [
        "import kashgari\n",
        "import jieba\n",
        "import pandas as pd\n",
        "from kashgari.tasks.classification import BiLSTM_Model\n",
        "from kashgari.embeddings import BERTEmbedding\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level = \"DEBUG\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:CUDA GPU available, you can set `kashgari.config.use_cudnn_cell = True` to use CuDNNCell. This will speed up the training, but will make model incompatible with CPU device.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhsQyvBLwa_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_file_path = \"~/datasets/data.csv\"\n",
        "\n",
        "data_frame = pd.read_csv(data_file_path, sep=\",\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYUspaRnwnFn",
        "colab_type": "code",
        "outputId": "9f1eb816-2a0e-42b2-fef2-060e4b00fd5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data_frame.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>距离川沙公路较近,但是公交指示不对,如果是\"蔡陆线\"的话,会非常麻烦.建议用别的路线.房间较...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>商务大床房，房间很大，床有2M宽，整体感觉经济实惠不错!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。房间本身很好。</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>宾馆在小街道上，不大好找，但还好北京热心同胞很多~宾馆设施跟介绍的差不多，房间很小，确实挺小...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>CBD中心,周围没什么店铺,说5星有点勉强.不知道为什么卫生间没有电吹风</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                                             review\n",
              "0      1  距离川沙公路较近,但是公交指示不对,如果是\"蔡陆线\"的话,会非常麻烦.建议用别的路线.房间较...\n",
              "1      1                       商务大床房，房间很大，床有2M宽，整体感觉经济实惠不错!\n",
              "2      1         早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。房间本身很好。\n",
              "3      1  宾馆在小街道上，不大好找，但还好北京热心同胞很多~宾馆设施跟介绍的差不多，房间很小，确实挺小...\n",
              "4      1               CBD中心,周围没什么店铺,说5星有点勉强.不知道为什么卫生间没有电吹风"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ewF1XrNxN2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_frame[\"cutted\"] = data_frame[\"review\"].apply(lambda x : list(jieba.cut(str(x))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJPc70_uXLxG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "55be9a1a-afee-41db-a76e-68bdf1dde75d"
      },
      "source": [
        "data_frame.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>review</th>\n",
              "      <th>cutted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>距离川沙公路较近,但是公交指示不对,如果是\"蔡陆线\"的话,会非常麻烦.建议用别的路线.房间较...</td>\n",
              "      <td>[距离, 川沙, 公路, 较近, ,, 但是, 公交, 指示, 不, 对, ,, 如果, 是...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>商务大床房，房间很大，床有2M宽，整体感觉经济实惠不错!</td>\n",
              "      <td>[商务, 大床, 房, ，, 房间, 很大, ，, 床有, 2M, 宽, ，, 整体, 感觉...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。房间本身很好。</td>\n",
              "      <td>[早餐, 太, 差, ，, 无论, 去, 多少, 人, ，, 那边, 也, 不加, 食品, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>宾馆在小街道上，不大好找，但还好北京热心同胞很多~宾馆设施跟介绍的差不多，房间很小，确实挺小...</td>\n",
              "      <td>[宾馆, 在, 小, 街道, 上, ，, 不大好, 找, ，, 但, 还好, 北京, 热心,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>CBD中心,周围没什么店铺,说5星有点勉强.不知道为什么卫生间没有电吹风</td>\n",
              "      <td>[CBD, 中心, ,, 周围, 没什么, 店铺, ,, 说, 5, 星, 有点, 勉强, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  ...                                             cutted\n",
              "0      1  ...  [距离, 川沙, 公路, 较近, ,, 但是, 公交, 指示, 不, 对, ,, 如果, 是...\n",
              "1      1  ...  [商务, 大床, 房, ，, 房间, 很大, ，, 床有, 2M, 宽, ，, 整体, 感觉...\n",
              "2      1  ...  [早餐, 太, 差, ，, 无论, 去, 多少, 人, ，, 那边, 也, 不加, 食品, ...\n",
              "3      1  ...  [宾馆, 在, 小, 街道, 上, ，, 不大好, 找, ，, 但, 还好, 北京, 热心,...\n",
              "4      1  ...  [CBD, 中心, ,, 周围, 没什么, 店铺, ,, 说, 5, 星, 有点, 勉强, ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChNUKC6FZodK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2a9eaed8-ae96-455c-fb86-9aa0531865b1"
      },
      "source": [
        "# 平衡数据\n",
        "df_positive = data_frame[data_frame.label == 1]\n",
        "df_negative = data_frame[data_frame.label == 0]\n",
        "\n",
        "half_size = len(df_positive) if len(df_positive) < len(df_negative) else len(df_negative)\n",
        "\n",
        "df_balanced = pd.concat([df_positive.sample(half_size, replace=df_positive.shape[0] < half_size), df_negative.sample(half_size, replace=df_negative.shape[0] < half_size)])\n",
        "\n",
        "print(len(df_balanced[df_balanced.label == 1]), len(df_balanced[df_balanced.label == 0]))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2444 2444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAm89F54bU-n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "9d46064a-cda7-4a72-f92b-879f88567200"
      },
      "source": [
        "df_balanced.sample(5)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>review</th>\n",
              "      <th>cutted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7384</th>\n",
              "      <td>0</td>\n",
              "      <td>住的标准单人间实在是小，房间里一张大床已经塞得满满的了，设施极其简陋除了电视机和烧水壶其他什...</td>\n",
              "      <td>[住, 的, 标准, 单人间, 实在, 是, 小, ，, 房间, 里, 一张, 大床, 已经...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6538</th>\n",
              "      <td>0</td>\n",
              "      <td>作为这个价位的经济型酒店，总体还算可以。但是房间构局极不合理，一个小桌子紧挨着床，上网必须盘...</td>\n",
              "      <td>[作为, 这个, 价位, 的, 经济型, 酒店, ，, 总体, 还, 算, 可以, 。, 但...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5932</th>\n",
              "      <td>0</td>\n",
              "      <td>大厅雄伟,但前台和房间太小、(走廊太长)房间分布太散，找同伴很累。如果能将宾馆分为2个馆,前...</td>\n",
              "      <td>[大厅, 雄伟, ,, 但, 前台, 和, 房间, 太小, 、, (, 走廊, 太, 长, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3739</th>\n",
              "      <td>1</td>\n",
              "      <td>到达绍兴后，只要给这个客栈一个电话，十分钟之内就会有一辆小面的来接你，很方便。“老台门客栈”...</td>\n",
              "      <td>[到达, 绍兴, 后, ，, 只要, 给, 这个, 客栈, 一个, 电话, ，, 十分钟, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6360</th>\n",
              "      <td>0</td>\n",
              "      <td>前台接待态度生硬，楼道内因很多剧组，脏乱不堪。房间内部面积很大（大床房），但是家具很少，而且...</td>\n",
              "      <td>[前台, 接待, 态度生硬, ，, 楼道, 内因, 很多, 剧组, ，, 脏乱, 不堪, 。...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      label  ...                                             cutted\n",
              "7384      0  ...  [住, 的, 标准, 单人间, 实在, 是, 小, ，, 房间, 里, 一张, 大床, 已经...\n",
              "6538      0  ...  [作为, 这个, 价位, 的, 经济型, 酒店, ，, 总体, 还, 算, 可以, 。, 但...\n",
              "5932      0  ...  [大厅, 雄伟, ,, 但, 前台, 和, 房间, 太小, 、, (, 走廊, 太, 长, ...\n",
              "3739      1  ...  [到达, 绍兴, 后, ，, 只要, 给, 这个, 客栈, 一个, 电话, ，, 十分钟, ...\n",
              "6360      0  ...  [前台, 接待, 态度生硬, ，, 楼道, 内因, 很多, 剧组, ，, 脏乱, 不堪, 。...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdD0pZQqblxT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fb64532f-63c3-4b0a-93b0-1f4930b560a0"
      },
      "source": [
        "df_balanced.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4888, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmYNpp64XQDG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "b6bfa64d-d4fa-481f-90b3-11f2c5636324"
      },
      "source": [
        "total = df_balanced.shape[0]\n",
        "\n",
        "print(\"total = {}\".format(total))\n",
        "\n",
        "random_frame = df_balanced.sample(df_balanced.shape[0])\n",
        "\n",
        "random_frame.head()\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total = 4888\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>review</th>\n",
              "      <th>cutted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3030</th>\n",
              "      <td>1</td>\n",
              "      <td>我的国外客人十分满意，我本人对大堂和门童也是很满意。</td>\n",
              "      <td>[我, 的, 国外, 客人, 十分满意, ，, 我, 本人, 对, 大堂, 和, 门童, 也...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7074</th>\n",
              "      <td>0</td>\n",
              "      <td>提早预定的房间3间标房4夜，到了酒店,看样子是个刚改造的样子,很多地方还留着小旅馆的舍不得仍...</td>\n",
              "      <td>[提早, 预定, 的, 房间, 3, 间, 标房, 4, 夜, ，, 到, 了, 酒店, ,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5551</th>\n",
              "      <td>0</td>\n",
              "      <td>硬件还算过得去。就是服务水准太差了。前台服务员好像都是煞笔，一个个装傻充愣，一问三不知！！另...</td>\n",
              "      <td>[硬件, 还, 算, 过得去, 。, 就是, 服务, 水准, 太差, 了, 。, 前台, 服...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6602</th>\n",
              "      <td>0</td>\n",
              "      <td>因为到达时间有点晚，所以携程的房间取消了。只好拿了2个套房和4个单间。房间是日式的，面积确实...</td>\n",
              "      <td>[因为, 到达, 时间, 有点, 晚, ，, 所以, 携程, 的, 房间, 取消, 了, 。...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1287</th>\n",
              "      <td>1</td>\n",
              "      <td>如果中餐再丰富一些就好了，一个人吃中餐没有找到合适的地方。西餐不错</td>\n",
              "      <td>[如果, 中餐, 再, 丰富, 一些, 就, 好, 了, ，, 一个, 人, 吃, 中餐, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      label  ...                                             cutted\n",
              "3030      1  ...  [我, 的, 国外, 客人, 十分满意, ，, 我, 本人, 对, 大堂, 和, 门童, 也...\n",
              "7074      0  ...  [提早, 预定, 的, 房间, 3, 间, 标房, 4, 夜, ，, 到, 了, 酒店, ,...\n",
              "5551      0  ...  [硬件, 还, 算, 过得去, 。, 就是, 服务, 水准, 太差, 了, 。, 前台, 服...\n",
              "6602      0  ...  [因为, 到达, 时间, 有点, 晚, ，, 所以, 携程, 的, 房间, 取消, 了, 。...\n",
              "1287      1  ...  [如果, 中餐, 再, 丰富, 一些, 就, 好, 了, ，, 一个, 人, 吃, 中餐, ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8yfZ6NYbw-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "split_point = [total * 70 // 100, total * 85 // 100]\n",
        "\n",
        "train_x = list(random_frame[\"cutted\"][:split_point[0]])\n",
        "train_y = list(random_frame[\"label\"][:split_point[0]])\n",
        "\n",
        "valid_x = list(random_frame[\"cutted\"][split_point[0]: split_point[1]])\n",
        "valid_y = list(random_frame[\"label\"][split_point[0]: split_point[1]])\n",
        "\n",
        "test_x = list(random_frame[\"cutted\"][split_point[1]:])\n",
        "test_y = list(random_frame[\"label\"][split_point[1]:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7ySjJ8_jFkb",
        "colab_type": "code",
        "outputId": "08f06396-851f-42c2-c6ac-bc00b85d448a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "# 下载 bert 预训练模型\n",
        "!mkdir -p ~/bert_models\n",
        "!wget https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip -O ~/bert_models/chinese_L-12_H-768_A-12.zip\n",
        "!cd ~/bert_models && unzip chinese_L-12_H-768_A-12.zip"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-04 03:05:29--  https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.124.128, 2607:f8b0:4001:c05::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.124.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 381892918 (364M) [application/zip]\n",
            "Saving to: ‘/root/bert_models/chinese_L-12_H-768_A-12.zip’\n",
            "\n",
            "/root/bert_models/c 100%[===================>] 364.20M  78.0MB/s    in 4.7s    \n",
            "\n",
            "2019-11-04 03:05:34 (78.0 MB/s) - ‘/root/bert_models/chinese_L-12_H-768_A-12.zip’ saved [381892918/381892918]\n",
            "\n",
            "Archive:  chinese_L-12_H-768_A-12.zip\n",
            "   creating: chinese_L-12_H-768_A-12/\n",
            "  inflating: chinese_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: chinese_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: chinese_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: chinese_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: chinese_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY-lXXf7Q5Rn",
        "colab_type": "code",
        "outputId": "3e590c6a-8356-4392-b9e9-f9524fdba518",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "bert_embedding = BERTEmbedding(\"/root/bert_models/chinese_L-12_H-768_A-12\",\n",
        "                               task=kashgari.CLASSIFICATION,\n",
        "                               sequence_length=64)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:seq_len: 64\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUlHZr7bjMe2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c61c4e07-29e5-4289-b015-ea70364e9e44"
      },
      "source": [
        "model = BiLSTM_Model(bert_embedding)\n",
        "model.fit(train_x, train_y, valid_x, valid_y, batch_size=1024, epochs=5)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_20\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Input-Token (InputLayer)        [(None, 64)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Segment (InputLayer)      [(None, 64)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token (TokenEmbedding [(None, 64, 768), (2 16226304    Input-Token[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Segment (Embedding)   (None, 64, 768)      1536        Input-Segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token-Segment (Add)   (None, 64, 768)      0           Embedding-Token[0][0]            \n",
            "                                                                 Embedding-Segment[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Position (PositionEmb (None, 64, 768)      49152       Embedding-Token-Segment[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Dropout (Dropout)     (None, 64, 768)      0           Embedding-Position[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Norm (LayerNormalizat (None, 64, 768)      1536        Embedding-Dropout[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 64, 768)      2362368     Embedding-Norm[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 64, 768)      0           Encoder-1-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 64, 768)      0           Embedding-Norm[0][0]             \n",
            "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 64, 768)      1536        Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward (FeedForw (None, 64, 768)      4722432     Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Dropout ( (None, 64, 768)      0           Encoder-1-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Add (Add) (None, 64, 768)      0           Encoder-1-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Norm (Lay (None, 64, 768)      1536        Encoder-1-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 64, 768)      2362368     Encoder-1-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 64, 768)      0           Encoder-2-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 64, 768)      0           Encoder-1-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 64, 768)      1536        Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward (FeedForw (None, 64, 768)      4722432     Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Dropout ( (None, 64, 768)      0           Encoder-2-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Add (Add) (None, 64, 768)      0           Encoder-2-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Norm (Lay (None, 64, 768)      1536        Encoder-2-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 64, 768)      2362368     Encoder-2-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 64, 768)      0           Encoder-3-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 64, 768)      0           Encoder-2-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 64, 768)      1536        Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward (FeedForw (None, 64, 768)      4722432     Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Dropout ( (None, 64, 768)      0           Encoder-3-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Add (Add) (None, 64, 768)      0           Encoder-3-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Norm (Lay (None, 64, 768)      1536        Encoder-3-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 64, 768)      2362368     Encoder-3-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 64, 768)      0           Encoder-4-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 64, 768)      0           Encoder-3-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 64, 768)      1536        Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward (FeedForw (None, 64, 768)      4722432     Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Dropout ( (None, 64, 768)      0           Encoder-4-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Add (Add) (None, 64, 768)      0           Encoder-4-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Norm (Lay (None, 64, 768)      1536        Encoder-4-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 64, 768)      2362368     Encoder-4-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 64, 768)      0           Encoder-5-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 64, 768)      0           Encoder-4-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 64, 768)      1536        Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward (FeedForw (None, 64, 768)      4722432     Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Dropout ( (None, 64, 768)      0           Encoder-5-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Add (Add) (None, 64, 768)      0           Encoder-5-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Norm (Lay (None, 64, 768)      1536        Encoder-5-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 64, 768)      2362368     Encoder-5-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 64, 768)      0           Encoder-6-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 64, 768)      0           Encoder-5-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 64, 768)      1536        Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward (FeedForw (None, 64, 768)      4722432     Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Dropout ( (None, 64, 768)      0           Encoder-6-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Add (Add) (None, 64, 768)      0           Encoder-6-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Norm (Lay (None, 64, 768)      1536        Encoder-6-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 64, 768)      2362368     Encoder-6-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 64, 768)      0           Encoder-7-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 64, 768)      0           Encoder-6-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 64, 768)      1536        Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward (FeedForw (None, 64, 768)      4722432     Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Dropout ( (None, 64, 768)      0           Encoder-7-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Add (Add) (None, 64, 768)      0           Encoder-7-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Norm (Lay (None, 64, 768)      1536        Encoder-7-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 64, 768)      2362368     Encoder-7-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 64, 768)      0           Encoder-8-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 64, 768)      0           Encoder-7-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 64, 768)      1536        Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward (FeedForw (None, 64, 768)      4722432     Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Dropout ( (None, 64, 768)      0           Encoder-8-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Add (Add) (None, 64, 768)      0           Encoder-8-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Norm (Lay (None, 64, 768)      1536        Encoder-8-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 64, 768)      2362368     Encoder-8-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 64, 768)      0           Encoder-9-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 64, 768)      0           Encoder-8-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 64, 768)      1536        Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward (FeedForw (None, 64, 768)      4722432     Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Dropout ( (None, 64, 768)      0           Encoder-9-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Add (Add) (None, 64, 768)      0           Encoder-9-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Norm (Lay (None, 64, 768)      1536        Encoder-9-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 64, 768)      2362368     Encoder-9-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 64, 768)      0           Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 64, 768)      0           Encoder-9-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 64, 768)      1536        Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward (FeedFor (None, 64, 768)      4722432     Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Dropout  (None, 64, 768)      0           Encoder-10-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Add (Add (None, 64, 768)      0           Encoder-10-MultiHeadSelfAttention\n",
            "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Norm (La (None, 64, 768)      1536        Encoder-10-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 64, 768)      2362368     Encoder-10-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 64, 768)      0           Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 64, 768)      0           Encoder-10-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 64, 768)      1536        Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward (FeedFor (None, 64, 768)      4722432     Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Dropout  (None, 64, 768)      0           Encoder-11-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Add (Add (None, 64, 768)      0           Encoder-11-MultiHeadSelfAttention\n",
            "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Norm (La (None, 64, 768)      1536        Encoder-11-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 64, 768)      2362368     Encoder-11-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 64, 768)      0           Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 64, 768)      0           Encoder-11-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 64, 768)      1536        Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward (FeedFor (None, 64, 768)      4722432     Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Dropout  (None, 64, 768)      0           Encoder-12-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Add (Add (None, 64, 768)      0           Encoder-12-MultiHeadSelfAttention\n",
            "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Norm (La (None, 64, 768)      1536        Encoder-12-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-Output (Concatenate)    (None, 64, 3072)     0           Encoder-9-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-10-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-11-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-12-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "non_masking_layer_3 (NonMasking (None, 64, 3072)     0           Encoder-Output[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_4 (Bidirectional) (None, 256)          3277824     non_masking_layer_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 2)            514         bidirectional_4[0][0]            \n",
            "==================================================================================================\n",
            "Total params: 104,611,330\n",
            "Trainable params: 3,278,338\n",
            "Non-trainable params: 101,332,992\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/5\n",
            "3/4 [=====================>........] - ETA: 10s - loss: 2.1264 - acc: 0.5192Epoch 1/5\n",
            "4/4 [==============================] - 45s 11s/step - loss: 1.8854 - acc: 0.5221 - val_loss: 0.6670 - val_acc: 0.5989\n",
            "Epoch 2/5\n",
            "3/4 [=====================>........] - ETA: 8s - loss: 0.7816 - acc: 0.5251 Epoch 1/5\n",
            "4/4 [==============================] - 33s 8s/step - loss: 0.8041 - acc: 0.5200 - val_loss: 0.8258 - val_acc: 0.4802\n",
            "Epoch 3/5\n",
            "3/4 [=====================>........] - ETA: 8s - loss: 0.7483 - acc: 0.5251 Epoch 1/5\n",
            "4/4 [==============================] - 33s 8s/step - loss: 0.7388 - acc: 0.5297 - val_loss: 0.6689 - val_acc: 0.5825\n",
            "Epoch 4/5\n",
            "3/4 [=====================>........] - ETA: 8s - loss: 0.6744 - acc: 0.5713 Epoch 1/5\n",
            "4/4 [==============================] - 33s 8s/step - loss: 0.6746 - acc: 0.5732 - val_loss: 0.6668 - val_acc: 0.5839\n",
            "Epoch 5/5\n",
            "3/4 [=====================>........] - ETA: 8s - loss: 0.6733 - acc: 0.5983 Epoch 1/5\n",
            "4/4 [==============================] - 33s 8s/step - loss: 0.6749 - acc: 0.5981 - val_loss: 0.6623 - val_acc: 0.6153\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f24bafcec88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUN8RC60YRsG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "0724dfa1-bc19-4011-a2ec-7fd89b103352"
      },
      "source": [
        "model.evaluate(test_x, test_y, batch_size=512)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6543    0.4889    0.5596       360\n",
            "           1     0.6043    0.7513    0.6698       374\n",
            "\n",
            "    accuracy                         0.6226       734\n",
            "   macro avg     0.6293    0.6201    0.6147       734\n",
            "weighted avg     0.6288    0.6226    0.6158       734\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}